{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTfkpkw5OgQv"
   },
   "source": [
    "### 1. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Connect to google drive\n",
    "My google drive directory will be mounted locally on `~/gdrive` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clone the project repository from github\n",
    "This repos contains utility functions, for data processing, model definition and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/AlkaSaliss/DEmoClassi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Download data from kaggle\n",
    "To use the kaggle API we need to download a google credentials from our kaggle account and save it to google drive so that we can access to it from google colab.\n",
    "\n",
    "After connecting to kaggle we'll download the fer2013 face image datasets used for facial expression classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vx_gO5LZTZAS",
    "outputId": "720b6441-3fdc-4a8c-8a0d-d555d6149eef"
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import io, os\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "\n",
    "drive_service = build('drive', 'v3')\n",
    "results = drive_service.files().list(\n",
    "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
    "kaggle_api_key = results.get('files', [])\n",
    "\n",
    "filename = \"/content/.kaggle/kaggle.json\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
    "fh = io.FileIO(filename, 'wb')\n",
    "downloader = MediaIoBaseDownload(fh, request)\n",
    "done = False\n",
    "while done is False:\n",
    "    status, done = downloader.next_chunk()\n",
    "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
    "os.chmod(filename, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fAARwp0SUBKo"
   },
   "outputs": [],
   "source": [
    "# download the data\n",
    "if not os.path.exists('~/.kaggle'):\n",
    "    os.system('mkdir ~/.kaggle')\n",
    "\n",
    "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "6op8NeDUbXfl",
    "outputId": "d99e464f-d0b8-4cdd-8d7a-82681a9515b9"
   },
   "outputs": [],
   "source": [
    "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qyERVv9ariS"
   },
   "outputs": [],
   "source": [
    "# create the data directory if not exist\n",
    "if not os.path.exists('/content/data'):\n",
    "    os.mkdir('/content/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "AEVBd_IZa4y4",
    "outputId": "b0c586af-fdf9-4772-ea69-672e87409855"
   },
   "outputs": [],
   "source": [
    "# extract ferg2013 dataset in the /content/data/ directory\n",
    "!tar -zxvf /content/fer2013.tar.gz -C /content/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tWxlr6CcR1t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCD-gdOfOX3V"
   },
   "outputs": [],
   "source": [
    "# install cmake in order to compile some modules from source\n",
    "!apt-get -y install cmake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rbn5HdXqOoY5"
   },
   "source": [
    "### 1. Install needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lMA_y49QPTDw"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Installing the gpu version of dlib package\n",
    "! git clone https://github.com/davisking/dlib.git\n",
    "%cd dlib\n",
    "! mkdir build\n",
    "%cd build\n",
    "! cmake .. -DDLIB_USE_CUDA=1 -DUSE_AVX_INSTRUCTIONS=1\n",
    "! cmake --build .\n",
    "%cd ..\n",
    "! python setup.py install --yes USE_AVX_INSTRUCTIONS --yes DLIB_USE_CUDA\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1010
    },
    "colab_type": "code",
    "id": "mtwjwTgtPe-Y",
    "outputId": "5a046596-1683-408d-94c4-496e1bb913ee"
   },
   "outputs": [],
   "source": [
    "# installing imutils and pytorch, kaggle\n",
    "!pip install --upgrade imutils\n",
    "!pip install torchvision\n",
    "!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu90/torch_nightly.html\n",
    "!pip install kaggle\n",
    "#!pip install face_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEM4nd-qaWf7"
   },
   "source": [
    "### 2. Downloading the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-kIM8w-TZXN"
   },
   "source": [
    "Connecting to kaggle to donwload ferg2013 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sn6n7m-hZphr"
   },
   "source": [
    "Connect to drive to download jaffedbase dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "EQ06ddSQZogj",
    "outputId": "f93d808e-89ae-4475-d829-3c95f041198d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0uOXH4z17CnG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3D8NH3hidTTZ"
   },
   "source": [
    "### 3. Image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OipBsXHXdElS"
   },
   "outputs": [],
   "source": [
    "# package import \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "from scipy.misc.pilutil import imread, imresize, imsave\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil\n",
    "import gzip\n",
    "import glob\n",
    "import random\n",
    "from random import shuffle\n",
    "import tqdm\n",
    "import cv2\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import exposure\n",
    "import skimage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import h5py\n",
    "import time\n",
    "import copy\n",
    "import imutils\n",
    "import dlib\n",
    "from imutils.face_utils import FaceAligner, rect_to_bb\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQQxHFwu-xBe"
   },
   "outputs": [],
   "source": [
    "# !rm -r /content/dlib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Xne8ls-PNP9"
   },
   "outputs": [],
   "source": [
    "# import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mZkKvTWae2CA",
    "outputId": "fb38c97a-1c09-4d6c-b712-b1c1b8ce0451"
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7oitqUv2aO_8"
   },
   "source": [
    "Now let's extract the dataset into data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mm9V09WEe-Dm"
   },
   "source": [
    "#### 3.1 Loading and processing ferg2013 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1L7d13EzfCkr"
   },
   "outputs": [],
   "source": [
    "fer2013 = pd.read_csv('/content/data/fer2013/fer2013.csv')\n",
    "# print some information\n",
    "fer2013.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "tzGP4ikyfk_p",
    "outputId": "6f937894-a15d-41c6-a446-918ab196a217"
   },
   "outputs": [],
   "source": [
    "# print some information\n",
    "fer2013.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "LN5yOG9wf603",
    "outputId": "507f5981-c9c1-417b-c546-ba3edf46ba92"
   },
   "outputs": [],
   "source": [
    "# print some lines\n",
    "fer2013.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "NngDNZ2GgLCO",
    "outputId": "dbb98829-d977-408f-ec21-b8df3465c443"
   },
   "outputs": [],
   "source": [
    "# print the count for train - public - private sets\n",
    "fer2013.Usage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "hOTuC6Khij08",
    "outputId": "d14a3d1e-97e4-4a9a-9abb-27d80b81382c"
   },
   "outputs": [],
   "source": [
    "# print counts for facial expressions\n",
    "fer2013.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJ6Blp9sigZa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "pOEX_CCLi0Yd",
    "outputId": "5be9ce63-ee39-48e6-c7e1-30b33fcb2f13"
   },
   "outputs": [],
   "source": [
    "# check if there's missoing values\n",
    "fer2013.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ukr4on92jHIc"
   },
   "outputs": [],
   "source": [
    "# reshape the images to get 48x48 pixels images\n",
    "fer2013_array = fer2013.pixels.values\n",
    "fer2013_array = np.array([[int(pix) for pix in img.split()] \n",
    "                          for img in fer2013_array])\n",
    "fer2013_array = fer2013_array.reshape((fer2013_array.shape[0], 48, 48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "GvqVhtZKkIxU",
    "outputId": "b81f8614-2359-4620-ff80-b16cb8bc8370"
   },
   "outputs": [],
   "source": [
    "print('Number of samples : {}'.format(fer2013_array.shape[0]))\n",
    "print(\"Images shape : {}x{} \".format(fer2013_array.shape[1], fer2013_array.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJ33K7ejlXxk"
   },
   "outputs": [],
   "source": [
    "# get the labels\n",
    "fer2013_labels = fer2013.emotion.values\n",
    "# get the flag (train, private and public set)\n",
    "flags = fer2013.Usage.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9erjYNplL0B"
   },
   "source": [
    "plot some sample images for each expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yIbE6PoklBSw"
   },
   "outputs": [],
   "source": [
    "def plot_examples(label):\n",
    "    dict_label = {\n",
    "        0: 'Angry',\n",
    "        1: 'Disgust',\n",
    "        2: 'Fear',\n",
    "        3: 'Happy',\n",
    "        4: 'Sad',\n",
    "        5: 'Surprise',\n",
    "        6: 'Neutral'\n",
    "    }\n",
    "    \n",
    "    print('Images belonging to class:', dict_label[label])\n",
    "    inds1 = set(np.where(fer2013_labels==label)[0])\n",
    "    inds2 = set(np.where(flags=='Training')[0])\n",
    "    inds = list(inds1.intersection(inds2))\n",
    "    sample_inds = np.random.choice(inds, 9, replace=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=3,ncols=3,figsize=(20,10))\n",
    "    ax = ax.ravel()\n",
    "    for idx, e in enumerate(sample_inds):\n",
    "        img = fer2013_array[e]\n",
    "        ax[idx].imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "id": "siNAsCcDnphO",
    "outputId": "5835a31e-e883-4196-ba7e-af36f71747e1"
   },
   "outputs": [],
   "source": [
    "plot_examples(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "id": "ULUGP46cpNrr",
    "outputId": "079856fe-3ad3-4363-a461-92823a2ddbc9"
   },
   "outputs": [],
   "source": [
    "plot_examples(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "id": "FB5KWZevpTIf",
    "outputId": "36d29142-cea3-434f-cc69-55e3fd25f2ae"
   },
   "outputs": [],
   "source": [
    "plot_examples(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "id": "liEKqA89pbgm",
    "outputId": "d940941d-ee1c-4718-ce65-5257f53f7084"
   },
   "outputs": [],
   "source": [
    "plot_examples(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "id": "fw1YqmJEpf05",
    "outputId": "032666c7-8c52-43ba-eeff-536edea43d23"
   },
   "outputs": [],
   "source": [
    "plot_examples(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "id": "yhfbmUyuptrN",
    "outputId": "5dc01f65-ea05-4962-c95d-58ceea616672"
   },
   "outputs": [],
   "source": [
    "plot_examples(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "id": "Qw8ldLSzpz7e",
    "outputId": "4bbb178e-8bbd-4d56-8fd9-0622c5ca14e5"
   },
   "outputs": [],
   "source": [
    "plot_examples(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jR1F8hbhqF3J"
   },
   "source": [
    "#### 3.2 Loading and processing jaffe data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RRpYX6-asLmI"
   },
   "source": [
    "Each jaffe image has its label incorporated in its file name.\n",
    "Let's load the file names and explore the different expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wL8cCCl9sdOo"
   },
   "outputs": [],
   "source": [
    "paths_jaffe = sorted(glob.glob('/content/data/jaffe/*.tiff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kugML7qssvSC",
    "outputId": "817a0578-c835-4700-8cb1-6fca42adc329"
   },
   "outputs": [],
   "source": [
    "print('Number of images: {}'.format(len(paths_jaffe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mzmQ6TWs2VC"
   },
   "outputs": [],
   "source": [
    "# get the labels\n",
    "jaffe_labels = np.array([im.split('.')[1][:2] for im in paths_jaffe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "rAl51Hp0tUEk",
    "outputId": "78c0a8f6-0e5e-40b7-c80f-c7e7c4871ad3"
   },
   "outputs": [],
   "source": [
    "print('frequency for each expression')\n",
    "unique, counts = np.unique(jaffe_labels, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4inW1M-qo9L"
   },
   "source": [
    "Let's create a function that read a tiff file and returns its numpy representation along with its label.\n",
    "\n",
    "jaffe image label is incorporated in the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwiVXhcPrpQY"
   },
   "outputs": [],
   "source": [
    "def load_tiff_image(path):\n",
    "    dict_label = {\n",
    "        0: 'AN',\n",
    "        1: 'DI',\n",
    "        2: 'FE',\n",
    "        3: 'HA',\n",
    "        4: 'SA',\n",
    "        5: 'SU',\n",
    "        6: 'NE'\n",
    "    }\n",
    "    rev_dict = dict([(v, k) for k, v in dict_label.items()])\n",
    "    \n",
    "    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    label = rev_dict[path.split('.')[1][:2]]\n",
    "    \n",
    "    return (im, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b3qwi3hT1Jla",
    "outputId": "f30778ae-24c8-44a0-cf11-6950d787eda0"
   },
   "outputs": [],
   "source": [
    "jaffe_data = [load_tiff_image(p) for p in tqdm.tqdm(paths_jaffe)]\n",
    "jaffe_array = np.concatenate([np.expand_dims(item[0], 0) for item in jaffe_data])\n",
    "jaffe_labels = np.array([item[1] for item in jaffe_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YTAgsRDG2nSD",
    "outputId": "c070fddb-01d4-4ee7-92c0-7b2d02268e30"
   },
   "outputs": [],
   "source": [
    "assert len(jaffe_array) == len(jaffe_labels)\n",
    "print('jaffe data shape', jaffe_array.shape, jaffe_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Yes4oqD1FhH"
   },
   "outputs": [],
   "source": [
    "def plot_jaffe(label):\n",
    "    dict_label = {\n",
    "        0: 'Angry',\n",
    "        1: 'Disgust',\n",
    "        2: 'Fear',\n",
    "        3: 'Happy',\n",
    "        4: 'Sad',\n",
    "        5: 'Surprise',\n",
    "        6: 'Neutral'\n",
    "    }\n",
    "    \n",
    "    print('Images belonging to class:', dict_label[label])\n",
    "    inds = np.where(jaffe_labels==label)[0]\n",
    "    sample_inds = np.random.choice(inds, 9, replace=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=3,ncols=3,figsize=(20,10))\n",
    "    ax = ax.ravel()\n",
    "    for idx, e in enumerate(sample_inds):\n",
    "        img = jaffe_array[e]\n",
    "        ax[idx].imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "Ia4vkEvlu-m3",
    "outputId": "5d358e1b-d00b-4e5f-8496-b144d0422f78"
   },
   "outputs": [],
   "source": [
    "plot_jaffe(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "yww21ZzwvJWR",
    "outputId": "f2432e2d-75b3-4325-c557-9cb602a0f85d"
   },
   "outputs": [],
   "source": [
    "plot_jaffe(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "w-mMXySFqFGT",
    "outputId": "1a307d36-a089-4de8-a4c8-c4634613d935"
   },
   "outputs": [],
   "source": [
    "plot_jaffe(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "PchiOTow37S0",
    "outputId": "e8229e6b-9017-4c2f-e8e3-e0d4478f1a3f"
   },
   "outputs": [],
   "source": [
    "plot_jaffe(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "49pz7JOT4Anm",
    "outputId": "05123de6-3dca-41c4-906b-03c1486b8eb2"
   },
   "outputs": [],
   "source": [
    "plot_jaffe(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "MK8hyJaz4GNR",
    "outputId": "a27f73bc-ebc1-447c-9cfd-f0cabf92df1a"
   },
   "outputs": [],
   "source": [
    "plot_jaffe(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "wup7ChZC4LNl",
    "outputId": "e7671704-971b-4fb4-fa3e-f7241397eb8f"
   },
   "outputs": [],
   "source": [
    "plot_jaffe(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MpVojDJd4akF"
   },
   "source": [
    "#### 3.3 Further processing steps:\n",
    "* face alignement\n",
    "* image cropping\n",
    "* image resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZQB5-ge5Alp"
   },
   "source": [
    "Define a function that applies all these prcessing steps to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "e_pAmxgDBIeb",
    "outputId": "7248d6b2-5bfc-4ce7-b11b-c48f3a20a523"
   },
   "outputs": [],
   "source": [
    "# download the pretrained face_detector file for dlib\n",
    "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 -P /content/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqakKMTMBsnX"
   },
   "outputs": [],
   "source": [
    "!bzip2 -dk /content/data/shape_predictor_68_face_landmarks.dat.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HF6jixhxCP4X"
   },
   "outputs": [],
   "source": [
    "PATH_DETECTOR = '/content/data/shape_predictor_68_face_landmarks.dat'\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(PATH_DETECTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esei1E50QjUS"
   },
   "outputs": [],
   "source": [
    "# function to align faces and compute landmarks\n",
    "def align_and_crop(list_images, desiredFaceWidth=256, convert=True, resize=(800, 800)):\n",
    "    \n",
    "    # converting image to 8-bit\n",
    "    if convert:\n",
    "        print('--------------Converting images----------------')\n",
    "        list_images = list(np.uint8(list_images))\n",
    "    #resizing the  images\n",
    "    if resize:\n",
    "        print('--------------Resizing images----------------')\n",
    "        #list_images = [transform.resize(np.repeat(np.expand_dims(im, 2), 3, 2), resize).astype('float32')\n",
    "        #               for im in list_images]\n",
    "        list_images = [np.repeat(np.expand_dims(im, 2), 3, 2) \n",
    "                       for im in list_images]\n",
    "        \n",
    "    # list_images = list(list_images)\n",
    "    \n",
    "\n",
    "    \n",
    "    results = []\n",
    "    print('----------------- Face alignment-------------------')\n",
    "    for im in list_images:\n",
    "        gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
    "        # detect the face in the image\n",
    "        rects = detector(gray, 2)\n",
    "        if len(rects) > 0: # if we have at least one face detected\n",
    "            rects = rects[0] # consider the 1st face\n",
    "            faceAligned = fa.align(im, gray, rects) # align the face\n",
    "            \n",
    "            # cropping the image\n",
    "            gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
    "            rects = detector(faceAligned, 2)\n",
    "            if len(rects) > 0:\n",
    "                rects = rects[0]\n",
    "                (x, y, w, h) = rect_to_bb(rects) # get the bounding box coordinates\n",
    "                faceCropped = transform.resize(faceAligned[y: y + h, \n",
    "                                                     x: x + w, 0], (48, 48))\n",
    "                # shape = imutils.face_utils.shape_to_np(predictor(faceAligned, rects))\n",
    "                # compute the cropping coordinates\n",
    "                # center = shape[27]\n",
    "                # right_eye = np.mean(shape[42:48], axis=0).astype(int)\n",
    "                # alpha = right_eye[0] - center[0]\n",
    "                # new_x = np.floor(center[0] - 1.2 * alpha).astype(int)\n",
    "                # new_y = np.floor(center[1] - 1.3 * alpha).astype(int)\n",
    "                # new_w = np.ceil(2.4 * alpha).astype(int)\n",
    "                # new_h = np.ceil(4.5 * alpha).astype(int)\n",
    "                  \n",
    "                # crop the face and resize image to 48x48\n",
    "                # faceCropped = transform.resize(faceAligned[new_y: new_y + new_h, \n",
    "                #                                     new_x: new_x + new_w, 0], (48, 48))\n",
    "                print('OK 1')\n",
    "                results.append(faceCropped)\n",
    "            else:\n",
    "                print('OK 2')\n",
    "                # if no face is found in the aligned face, return just a resized aligned face 48x48\n",
    "                # results.append(transform.resize(faceAligned[:, :, 0], (48, 48)))\n",
    "                results.append(faceAligned[:, :, 0])\n",
    "        else:\n",
    "            print('OK 3')\n",
    "            # if no face is found at all in the image, return the resized original image 48x48\n",
    "            # results.append(transform.resize(im[:, :, 0], (48, 48)))\n",
    "            results.append(im[:, :, 0])\n",
    "    \n",
    "    # apply histogram equalization to all images and return the result\n",
    "    return np.concatenate(\n",
    "        [np.expand_dims(exposure.equalize_hist(item), 0) for item in results]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S19dOSKl6l6h"
   },
   "outputs": [],
   "source": [
    "# Instanciate a face aligner object\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=48)\n",
    "\n",
    "# function to process one image\n",
    "def align_and_crop_one(im):\n",
    "\n",
    "    gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
    "    # detect the face in the image\n",
    "    rects = detector(gray, 2)\n",
    "    if len(rects) > 0: # if we have at least one face detected\n",
    "        rects = rects[0] # consider the 1st face\n",
    "        faceAligned = fa.align(im, gray, rects) # align the face\n",
    "\n",
    "        # cropping the image\n",
    "        gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
    "        rects = detector(faceAligned, 2)\n",
    "        if len(rects) > 0:\n",
    "            rects = rects[0]\n",
    "            (x, y, w, h) = rect_to_bb(rects) # get the bounding box coordinates\n",
    "            faceCropped = transform.resize(faceAligned[y: y + h, \n",
    "                                                 x: x + w, 0], (48, 48))\n",
    "\n",
    "            # print('OK 1')\n",
    "            return (exposure.equalize_hist(faceCropped), 'OK1')\n",
    "        else:\n",
    "            # print('OK 2')\n",
    "            # if no face is found in the aligned face, return just a resized aligned face 48x48\n",
    "            return (exposure.equalize_hist(faceAligned[:, :, 0]), 'OK2')\n",
    "    else:\n",
    "        # print('OK 3')\n",
    "        # if no face is found at all in the image, return the resized original image 48x48\n",
    "        return (exposure.equalize_hist(transform.resize(im[:, :, 0], (48, 48))), 'OK3')\n",
    "\n",
    "\n",
    "# function to align faces and compute landmarks\n",
    "def align_and_crop(list_images, convert=True, reshape=True):\n",
    "    # converting image to 8-bit\n",
    "    if convert:\n",
    "        print('--------------Converting images----------------')\n",
    "        list_images = list(np.uint8(list_images))\n",
    "    #resizing the  images\n",
    "    if reshape:\n",
    "        print('--------------Resizing images----------------')\n",
    "        list_images = [np.repeat(np.expand_dims(im, 2), 3, 2) \n",
    "                       for im in list_images]\n",
    "    \n",
    "    with Pool() as p:\n",
    "        results_tuple = p.map(align_and_crop_one, tqdm.tqdm(list_images))\n",
    "    \n",
    "    results1 = [item[0] for item in results_tuple]\n",
    "    results2 = [item[1] for item in results_tuple]\n",
    "    \n",
    "    # apply histogram equalization to all images and return the result\n",
    "    return np.concatenate([np.expand_dims(item, 0) for item in results1]), dict(Counter(results2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "BNmLWsUj7q32",
    "outputId": "f8720203-a09e-4918-d67b-e872c74c347a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fer2013_transformed, counts = align_and_crop(fer2013_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "16wZuz-258ie",
    "outputId": "02d32e8e-6f3e-4291-ac8d-5d5eb16c328b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "jaffe_transformed, counts_j = align_and_crop(jaffe_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwlmTteQL14Z"
   },
   "source": [
    "Save the data to gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6VV1AhsMSIt"
   },
   "outputs": [],
   "source": [
    "#path to Google Drive\n",
    "PATH_DRIVE = '/content/gdrive/My Drive/Face_detection'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "FrMuNWQeJaQG",
    "outputId": "92c3bfcd-58b4-4d27-8cc1-ee2c85159010"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.save(os.path.join(PATH_DRIVE, 'fer2013_transformed.npy'), fer2013_transformed)\n",
    "np.save(os.path.join(PATH_DRIVE, 'fer2013_labels.npy'), fer2013_labels)\n",
    "np.save(os.path.join(PATH_DRIVE, 'fer2013_array.npy'), fer2013_array)\n",
    "np.save(os.path.join(PATH_DRIVE, 'flags.npy'), flags)\n",
    "\n",
    "np.save(os.path.join(PATH_DRIVE, 'jaffe_array.npy'), jaffe_array)\n",
    "np.save(os.path.join(PATH_DRIVE, 'jaffe_labels.npy'), jaffe_labels)\n",
    "np.save(os.path.join(PATH_DRIVE, 'jaffe_transformed.npy'), jaffe_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TW0IjYMirVD"
   },
   "source": [
    "### 4. Modeling phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "5t_Cd9E4hqDD",
    "outputId": "f2548aec-6315-4864-b5f2-5d7cb789e7d8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the data from drive\n",
    "PATH_DRIVE = '/content/gdrive/My Drive/Face_detection'\n",
    "\n",
    "fer2013_transformed = np.load(os.path.join(PATH_DRIVE, 'fer2013_transformed.npy'))\n",
    "fer2013_labels = np.load(os.path.join(PATH_DRIVE, 'fer2013_labels.npy'))\n",
    "fer2013_array = np.load(os.path.join(PATH_DRIVE, 'fer2013_array.npy'))\n",
    "flags = np.load(os.path.join(PATH_DRIVE, 'flags.npy'))\n",
    "\n",
    "jaffe_array = np.load(os.path.join(PATH_DRIVE, 'jaffe_array.npy'))\n",
    "jaffe_labels = np.load(os.path.join(PATH_DRIVE, 'jaffe_labels.npy'))\n",
    "jaffe_transformed = np.load(os.path.join(PATH_DRIVE, 'jaffe_transformed.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wufKN7Di3tw"
   },
   "source": [
    "#### 4.1 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVAZTZ1djrdc"
   },
   "source": [
    "Create train - validation - test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wr8nmUoEj3KO"
   },
   "outputs": [],
   "source": [
    "# train set\n",
    "train_fer2013_images = fer2013_transformed[np.where(flags=='Training')[0]]\n",
    "train_fer2013_labels = fer2013_labels[np.where(flags=='Training')[0]]\n",
    "\n",
    "# validation set\n",
    "val_fer2013_images = fer2013_transformed[np.where(flags=='PublicTest')[0]]\n",
    "val_fer2013_labels = fer2013_labels[np.where(flags=='PublicTest')[0]]\n",
    "\n",
    "# test set\n",
    "test_fer2013_images = fer2013_transformed[np.where(flags=='PrivateTest')[0]]\n",
    "test_fer2013_labels = fer2013_labels[np.where(flags=='PrivateTest')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "4ZSpQaKWkXKJ",
    "outputId": "6defbcf5-4324-4c64-f0d4-39b0982cc901"
   },
   "outputs": [],
   "source": [
    "print(train_fer2013_images.shape, train_fer2013_labels.shape)\n",
    "print(val_fer2013_images.shape, val_fer2013_labels.shape)\n",
    "print(test_fer2013_images.shape, test_fer2013_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ze6Yleodtk9"
   },
   "outputs": [],
   "source": [
    "def plot_images(images, labels, class_):\n",
    "    dict_label = {\n",
    "        0: 'Angry',\n",
    "        1: 'Disgust',\n",
    "        2: 'Fear',\n",
    "        3: 'Happy',\n",
    "        4: 'Sad',\n",
    "        5: 'Surprise',\n",
    "        6: 'Neutral'\n",
    "    }\n",
    "    \n",
    "    print('Images belonging to class:', dict_label[class_])\n",
    "    inds = np.where(labels==class_)[0]\n",
    "    sample_inds = np.random.choice(inds, 16, replace=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=4,ncols=4,figsize=(22,10))\n",
    "    ax = ax.ravel()\n",
    "    for idx, e in enumerate(sample_inds):\n",
    "        img = images[e]\n",
    "        ax[idx].imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "id": "GfHXSYwgeeJZ",
    "outputId": "0462cbd7-be96-4b6f-a7aa-2765d6fb5221"
   },
   "outputs": [],
   "source": [
    "plot_images(jaffe_transformed, jaffe_labels, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qj_Os-Adle0-"
   },
   "source": [
    "Now we concatenate fer2013 data and jaffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ckbRxdJlv8d"
   },
   "outputs": [],
   "source": [
    "train_images = np.concatenate([train_fer2013_images, jaffe_transformed])\n",
    "train_labels = np.concatenate([train_fer2013_labels, jaffe_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "xlrAMS5UmAIS",
    "outputId": "0fff2729-517b-4589-e4da-5b9d865e7acb"
   },
   "outputs": [],
   "source": [
    "print(train_images.shape, train_labels.shape)\n",
    "print(val_fer2013_images.shape, val_fer2013_labels.shape)\n",
    "print(test_fer2013_images.shape, test_fer2013_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "enO_Q_Rci--I"
   },
   "outputs": [],
   "source": [
    "# Let's create a pytorch dataset class\n",
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnR-Qx_oi1PF"
   },
   "outputs": [],
   "source": [
    "class FaceData(Dataset):\n",
    "    \"\"\"Face image dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        assert images.shape[0] == labels.shape[0]\n",
    "        self.images = images\n",
    "        self.labels = np.expand_dims(labels, 1)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = (np.expand_dims(self.images[idx], 0), self.labels[idx])\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "# Transformer to convert numpy into pytorch tensor\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        return (torch.from_numpy(sample[0]), torch.from_numpy(sample[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "i7-eFjtCBRJi"
   },
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "N_EPOCHS = 25\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y1-AqblBss9a"
   },
   "outputs": [],
   "source": [
    "# Instanciate a transform object\n",
    "list_transforms = transforms.Compose([ToTensor()])\n",
    "\n",
    "# Create train, test and validation data objects\n",
    "face_datasets = {\n",
    "    'train': FaceData(train_images, train_labels, list_transforms),\n",
    "    'val': FaceData(val_fer2013_images, val_fer2013_labels, list_transforms),\n",
    "}\n",
    "\n",
    "face_datasets_test = FaceData(test_fer2013_images, test_fer2013_labels, list_transforms)\n",
    "\n",
    "# Data loaders\n",
    "dataloaders = {\n",
    "    phase: DataLoader(face_datasets[phase], batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    for phase in ['train', 'val']\n",
    "}\n",
    "dataset_sizes = {phase: len(face_datasets[phase]) for phase in ['train', 'val']}\n",
    "\n",
    "dataloaders_test = DataLoader(face_datasets_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "dataset_sizes_test = len(face_datasets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "XK95jQOvyTQg",
    "outputId": "6a1d70fa-fe6d-4b98-8266-a22d38a29e12"
   },
   "outputs": [],
   "source": [
    "# check if train data loader is OK\n",
    "tmp = next(iter(dataloaders['train']))\n",
    "print(tmp[0].size())\n",
    "print(tmp[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "eHEeBPYt1h9A",
    "outputId": "11763c9c-88b3-402d-d778-1e7681575afc"
   },
   "outputs": [],
   "source": [
    "# check if validation data loader is OK\n",
    "tmp = next(iter(dataloaders['val']))\n",
    "print(tmp[0].size())\n",
    "print(tmp[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "cjZqse4l2DsI",
    "outputId": "9cb630fd-e990-47d2-a047-4c7480970535"
   },
   "outputs": [],
   "source": [
    "# check if test data loader is OK\n",
    "tmp = next(iter(dataloaders_test))\n",
    "print(tmp[0].size())\n",
    "print(tmp[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "NGgpygqUaMmv",
    "outputId": "3728f567-5005-49e4-b053-a27357e3c87a"
   },
   "outputs": [],
   "source": [
    "print(tmp[0].dtype)\n",
    "print(tmp[1].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFAY_5bA2RlC"
   },
   "source": [
    "#### 4.2 Create a model\n",
    "\n",
    "We'll implement a depthwise separable convolution network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsQ836exLeHo"
   },
   "source": [
    "Depthwise separable conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZrOZSMsLiZS"
   },
   "outputs": [],
   "source": [
    "class SeparableConv(torch.nn.Module):\n",
    "    \"\"\"Depthwise separable convolution layer implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nout, kernel_size=3):\n",
    "        super(SeparableConv, self).__init__()\n",
    "        self.depthwise = torch.nn.Conv2d(nin, nin, kernel_size=kernel_size, groups=nin)\n",
    "        self.pointwise = torch.nn.Conv2d(nin, nout, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_ZtmTrbLudz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OboEkDyL2QcN"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout=0.3, n_class=7, n_filters=[64, 128, 256, 512]):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.n_class = n_class\n",
    "        self.n_filters = n_filters\n",
    "        \n",
    "        # 1st block\n",
    "        self.conv1 = SeparableConv(1, self.n_filters[0])\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(self.n_filters[0])\n",
    "        self.conv2 = SeparableConv(self.n_filters[0], self.n_filters[0])\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(self.n_filters[0])\n",
    "        \n",
    "        # 2nd block\n",
    "        self.conv3 = SeparableConv(self.n_filters[0], self.n_filters[1])\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(self.n_filters[1])\n",
    "        self.conv4 = SeparableConv(self.n_filters[1], self.n_filters[1])\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(self.n_filters[1])\n",
    "        \n",
    "        # 3rd block\n",
    "        self.conv5 = SeparableConv(self.n_filters[1], self.n_filters[2])\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(self.n_filters[2])\n",
    "        self.conv6 = SeparableConv(self.n_filters[2], self.n_filters[2])\n",
    "        self.batchnorm6 = torch.nn.BatchNorm2d(self.n_filters[2])\n",
    "        \n",
    "        # 4th block\n",
    "        self.conv7 = SeparableConv(self.n_filters[2], self.n_filters[3])\n",
    "        self.batchnorm7 = torch.nn.BatchNorm2d(self.n_filters[3])\n",
    "        self.conv8 = SeparableConv(self.n_filters[3], self.n_filters[3])\n",
    "        self.batchnorm8 = torch.nn.BatchNorm2d(self.n_filters[3])\n",
    "        \n",
    "        self.avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 1st fc block\n",
    "        self.fc1 = torch.nn.Linear(self.n_filters[3], 256)\n",
    "        self.batchnorm9 = torch.nn.BatchNorm1d(256)\n",
    "        \n",
    "         # 2nd fc block\n",
    "        self.fc2 = torch.nn.Linear(256, 128)\n",
    "        self.batchnorm10 = torch.nn.BatchNorm1d(128)\n",
    "        \n",
    "         # output block\n",
    "        self.fc3 = torch.nn.Linear(128, self.n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #1st block\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # 2nd block\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.batchnorm3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.batchnorm4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # 3rd block\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.batchnorm5(x))\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(self.batchnorm6(x))\n",
    "        # x = F.max_pool2d(x, 2)\n",
    "                \n",
    "        # 4th block\n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(self.batchnorm7(x))\n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(self.batchnorm8(x))\n",
    "        # x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = F.dropout(x.view(-1, x.size()[1]), self.dropout)\n",
    "        \n",
    "        x = F.relu(self.batchnorm9(self.fc1(x)))\n",
    "        x = F.dropout(x, self.dropout)\n",
    "        \n",
    "        x = F.relu(self.batchnorm10(self.fc2(x)))\n",
    "        x = F.dropout(x, self.dropout)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tivI63J4XZG9"
   },
   "source": [
    "#### 4. 3 Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q1H5rv5z2QTn",
    "outputId": "a378117a-cf57-4d43-f774-23390d06697c"
   },
   "outputs": [],
   "source": [
    "# check if GPU available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXWCt8gA2QLY"
   },
   "outputs": [],
   "source": [
    "# instantiate model and optimizers\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "colab_type": "code",
    "id": "mRox0tWR2QCQ",
    "outputId": "a693d707-8b39-4bbc-c216-23838f0c2799"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1A86WCBZmhe"
   },
   "outputs": [],
   "source": [
    "# utility function to get the number of parameters in a model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Re-rItDbZx4-",
    "outputId": "070ce364-9847-4a47-ede8-ee346a82042a"
   },
   "outputs": [],
   "source": [
    "print('Number of trainable paramaters: {}'.format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IK6CzsYfX2tj"
   },
   "source": [
    "Let's create a training utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p48KzaP8YIWQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwEGl4e_2P50"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, data=dataloaders, dataset_sizes=dataset_sizes, num_epochs=N_EPOCHS):\n",
    "    since = time.time()\n",
    "    acc_history = {'train': [], 'val': []}\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # set model to training mode\n",
    "            else:\n",
    "                model.eval()   # set model to evaluation mode\n",
    "        \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, labels in data[phase]:\n",
    "                # inputs = inputs.to(device, dtype=torch.float)\n",
    "                inputs = inputs.to(device, dtype=torch.float)\n",
    "                labels = labels.view(labels.size()[0]).to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward pass\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    # loss = criterion(outputs, labels)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward pass + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            acc_history[phase].append(epoch_acc.to('cpu').numpy().mean()) \n",
    "            loss_history[phase].append(epoch_loss) \n",
    "            \n",
    "            print('{} Loss: {:.4f}, Acc: {:.4f}'.format(\n",
    "            phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # deep copy th emodel\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, acc_history, loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpDe1L8zEfQC"
   },
   "outputs": [],
   "source": [
    "# helper plot function to plot the training history\n",
    "def plot_train_history(accuracies, losses):\n",
    "    acc = accuracies['train']\n",
    "    val_acc = accuracies['val']\n",
    "    loss = losses['train']\n",
    "    val_loss = losses['val']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2223
    },
    "colab_type": "code",
    "id": "aPj0BPKQ2Pza",
    "outputId": "fb43a6ae-e454-4a99-ec41-2154e290f53c"
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "model, accuracies, losses = train_model(model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "colab_type": "code",
    "id": "cG7BIbEAEYhJ",
    "outputId": "a30b316e-01ad-46b6-e39b-c5696ffce49c"
   },
   "outputs": [],
   "source": [
    "plot_train_history(accuracies, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "skEiiG4L14xT"
   },
   "source": [
    "Clearly the model is overfitting, let's increase dropout rate from 30 to 70%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJOzorPu2PlP"
   },
   "outputs": [],
   "source": [
    "# instantiate model and optimizers\n",
    "model2 = Model(0.7)\n",
    "model2 = model2.to(device)\n",
    "criterion2 = torch.nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4393
    },
    "colab_type": "code",
    "id": "tdOlTdXV2Pef",
    "outputId": "472c2586-5753-476f-b427-0c61329be7e9"
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "model2, accuracies2, losses2 = train_model(model2, criterion2, optimizer2, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "colab_type": "code",
    "id": "5K5U8kdE2PYK",
    "outputId": "ba1450ee-cdf0-4fee-fb55-4420498f3a60"
   },
   "outputs": [],
   "source": [
    "plot_train_history(accuracies2, losses2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9zOiWxcK82b"
   },
   "source": [
    "The model is still overfitting, let's add L2 regularization to model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNdXs4ZX2PRZ"
   },
   "outputs": [],
   "source": [
    "# instantiate model and optimizers\n",
    "model3 = Model(0.7)\n",
    "model3 = model3.to(device)\n",
    "criterion3 = torch.nn.CrossEntropyLoss()\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=1e-2, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4393
    },
    "colab_type": "code",
    "id": "yJkdTi5j2PJN",
    "outputId": "8bb7d7c8-629e-41d7-9c34-d84db59f1a6e"
   },
   "outputs": [],
   "source": [
    "model3, accuracies3, losses3 = train_model(model3, criterion3, optimizer3, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "colab_type": "code",
    "id": "oaJk9y5dkZF4",
    "outputId": "28e8e253-81a2-43e1-a30c-b4fc735ceeb7"
   },
   "outputs": [],
   "source": [
    "plot_train_history(accuracies3, losses3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33TT5Q_w2O_y"
   },
   "outputs": [],
   "source": [
    "# instantiate model and optimizers\n",
    "model4 = Model(0.7)\n",
    "model4 = model4.to(device)\n",
    "criterion4 = torch.nn.CrossEntropyLoss()\n",
    "optimizer4 = optim.Adam(model4.parameters(), weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4393
    },
    "colab_type": "code",
    "id": "wl2jHCgk2OzD",
    "outputId": "45fef5f5-1623-461c-b617-a1161023df56"
   },
   "outputs": [],
   "source": [
    "model4, accuracies4, losses4 = train_model(model4, criterion4, optimizer4, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "colab_type": "code",
    "id": "D1Dznvli4xYP",
    "outputId": "09899746-a8bf-493c-db83-ffb253f9370e"
   },
   "outputs": [],
   "source": [
    "plot_train_history(accuracies4, losses4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x7fMe1c6mzrw"
   },
   "source": [
    "#### 4.4 Exploring model predictions\n",
    "\n",
    "Validation accuracy is varying between 53-55%. Let's explore the predictions by the model to see where it is doing well and where it is failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-puCNHSQnrq8"
   },
   "outputs": [],
   "source": [
    "# an evaluation funtion\n",
    "\n",
    "labels_ = [0, 1, 2, 3, 4, 5, 6]\n",
    "target_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "def evaluate_model(model=model, data=dataloaders['val'], \n",
    "                   title='Confusion matrix', normalize=False,\n",
    "                  labels_=labels_, target_names=target_names):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # first, get the predictions\n",
    "    model.eval() # set model in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in data:\n",
    "            # inputs = inputs.to(device, dtype=torch.float)\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            y_true.append(labels.squeeze(1))\n",
    "            labels = labels.view(labels.size()[0]).to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_pred.append(preds.to('cpu').numpy())\n",
    "            \n",
    "    # print classification report\n",
    "    y_true, y_pred = np.concatenate(y_true), np.concatenate(y_pred)\n",
    "    print(classification_report(y_true, y_pred, labels=labels_, target_names=target_names))\n",
    "    \n",
    "    # plot the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        \n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "colab_type": "code",
    "id": "3RJcmGzPnUSG",
    "outputId": "ef0d78dc-8426-4db6-9e15-eb7ab93c1915"
   },
   "outputs": [],
   "source": [
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RyLhCEL7b3g6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "colab_type": "code",
    "id": "gPKTK8Vi2EDb",
    "outputId": "4000e743-0990-463e-f09c-4f76aa31ef83"
   },
   "outputs": [],
   "source": [
    "evaluate_model(model=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "colab_type": "code",
    "id": "Ag9f6OkTbT7r",
    "outputId": "4d83dcc5-c588-422a-ed84-9c73374f9dd8"
   },
   "outputs": [],
   "source": [
    "evaluate_model(model=model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "colab_type": "code",
    "id": "ulU10Ohlv7Ra",
    "outputId": "1423498f-332b-4a44-91eb-3754fed676af"
   },
   "outputs": [],
   "source": [
    "evaluate_model(model=model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxY91XJ1dOWz"
   },
   "outputs": [],
   "source": [
    "# Let's save the trained models\n",
    "if not os.path.exists('/content/gdrive/My Drive/Face_detection/phase_1'):\n",
    "    os.mkdir('/content/gdrive/My Drive/Face_detection/phase_1')\n",
    "\n",
    "PATH_TMP = '/content/gdrive/My Drive/Face_detection/phase_1'\n",
    "model_names = ['model', 'model2', 'model3', 'model4']\n",
    "list_models = [model, model2, model3, model4]\n",
    "for i in range(len(list_models)):\n",
    "    torch.save(list_models[i].state_dict(), os.path.join(PATH_TMP, model_names[i]+'.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmcYx8UrcFEC"
   },
   "source": [
    "From the above evaluations, we can see that the worst classes in terms of recall are \"Disgust\" and \"Fear\" . Let's remove these 2 classes and see How the model will perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QU0V_LXTzINU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iaAklpiMi-rm"
   },
   "outputs": [],
   "source": [
    "# remove Disgust and Fear classes\n",
    "train_images_new = train_images[np.where((train_labels!=1) & (train_labels!=2))[0]]\n",
    "train_labels_new = train_labels[np.where((train_labels!=1) & (train_labels!=2))[0]]\n",
    "\n",
    "test_images_new = test_fer2013_images[np.where((test_fer2013_labels!=1) & (test_fer2013_labels!=2))[0]]\n",
    "test_labels_new = test_fer2013_labels[np.where((test_fer2013_labels!=1) & (test_fer2013_labels!=2))[0]]\n",
    "\n",
    "val_images_new = val_fer2013_images[np.where((val_fer2013_labels!=1) & (val_fer2013_labels!=2))[0]]\n",
    "val_labels_new = val_fer2013_labels[np.where((val_fer2013_labels!=1) & (val_fer2013_labels!=2))[0]]\n",
    "\n",
    "# Replace the other labels values to have a coherent 0 1 2, ... labelling\n",
    "new_labels = {0: 0, 3: 1, 4: 2, 5: 3, 6: 4}\n",
    "train_labels_new = np.array([new_labels[item] for item in train_labels_new])\n",
    "test_labels_new = np.array([new_labels[item] for item in test_labels_new])\n",
    "val_labels_new = np.array([new_labels[item] for item in val_labels_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "hvngRusvzH91",
    "outputId": "ca2a702b-ef19-480a-fb6b-a88717c71272"
   },
   "outputs": [],
   "source": [
    "print(train_images_new.shape, train_labels_new.shape)\n",
    "print(val_images_new.shape, val_labels_new.shape)\n",
    "print(test_images_new.shape, test_labels_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8vTDa_2C70XS",
    "outputId": "734d131b-e014-46cf-be60-237a015f6e8b"
   },
   "outputs": [],
   "source": [
    "24328/(3037+3006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sMQ21JsdlFii",
    "outputId": "9aac9314-9c9b-4ec5-e1c0-9822170ea12c"
   },
   "outputs": [],
   "source": [
    "np.unique(val_labels_new, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BD_Lz2tRiX2x"
   },
   "outputs": [],
   "source": [
    "# Instanciate a transform object\n",
    "\n",
    "# Create train, test and validation data objects\n",
    "face_datasets_new = {\n",
    "    'train': FaceData(train_images_new, train_labels_new, list_transforms),\n",
    "    'val': FaceData(val_images_new, val_labels_new, list_transforms),\n",
    "}\n",
    "\n",
    "face_datasets_test_new = FaceData(test_images_new, test_labels_new, list_transforms)\n",
    "\n",
    "# Data loaders\n",
    "dataloaders_new = {\n",
    "    phase: DataLoader(face_datasets_new[phase], batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    for phase in ['train', 'val']\n",
    "}\n",
    "dataset_sizes_new = {phase: len(face_datasets_new[phase]) for phase in ['train', 'val']}\n",
    "\n",
    "dataloaders_test_new = DataLoader(face_datasets_test_new, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "dataset_sizes_test_new = len(face_datasets_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JbgD8RUsnu48"
   },
   "outputs": [],
   "source": [
    "# instanciate a new model with 5 classes\n",
    "model5 = Model(0.7, 5)\n",
    "model5 = model5.to(device)\n",
    "criterion5 = torch.nn.CrossEntropyLoss()\n",
    "optimizer5 = optim.Adam(model5.parameters(), weight_decay=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4393
    },
    "colab_type": "code",
    "id": "uLqDfx-1pJCk",
    "outputId": "cc0752cd-4d32-4c40-91e9-103c2bb8d672"
   },
   "outputs": [],
   "source": [
    "model5, accuracies5, losses5 = train_model(model5, criterion5, optimizer5, \n",
    "                                           data=dataloaders_new,\n",
    "                                           dataset_sizes=dataset_sizes_new,\n",
    "                                           num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "colab_type": "code",
    "id": "yeg3U5bIFW8V",
    "outputId": "f813b1a5-1316-467b-f5f9-fc4942bd217a"
   },
   "outputs": [],
   "source": [
    "plot_train_history(accuracies5, losses5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "colab_type": "code",
    "id": "ZCyQ0lcNFmlW",
    "outputId": "083e556f-4ab7-436e-b8ef-1581da1d2031"
   },
   "outputs": [],
   "source": [
    "labels_ = [0, 1, 2, 3, 4]\n",
    "target_names = ['Angry', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "evaluate_model(model=model5, data=dataloaders_new['val'], labels_=labels_, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Bm1xZtm0kqQ"
   },
   "outputs": [],
   "source": [
    "PATH_TMP = '/content/gdrive/My Drive/Face_detection/phase_1'\n",
    "torch.save(model5.state_dict(), os.path.join(PATH_TMP, 'model5.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_XaV_b31Hvu"
   },
   "source": [
    "### 5. Feature maps visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yp-5aI-M1nnj"
   },
   "outputs": [],
   "source": [
    "class CamExtractor:\n",
    "    \n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "    \n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients = grad\n",
    "    \n",
    "    def forward_pass_on_convolutions(self, x):\n",
    "        conv_output = None\n",
    "        for module_name, module in self.model._modules.items():\n",
    "            print(module_name)\n",
    "            if module_name == 'fc1':\n",
    "                x = x.view(-1, x.size()[1])\n",
    "            if module_name == 'fc3':\n",
    "                return conv_output, x\n",
    "            x = module(x) # forward pass\n",
    "            if module_name == self.target_layer:\n",
    "                print('OK')\n",
    "                x.register_hook(self.save_gradient)\n",
    "                conv_output = x\n",
    "        \n",
    "        return conv_output, x\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        conv_output, x = self.forward_pass_on_convolutions(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = self.model.fc3(x)\n",
    "        return conv_output, x\n",
    "\n",
    "class GradCam:\n",
    "    \n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.extractor = CamExtractor(self.model, target_layer)\n",
    "    \n",
    "    def generate_cam(self, input_image, target_index=None):\n",
    "        conv_output, model_output = self.extractor.forward_pass(input_image)\n",
    "        if target_index == None:\n",
    "            target_index = np.argmax(model_output.data.numpy())\n",
    "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_().to(device)\n",
    "        one_hot_output[0][target_index] = 1\n",
    "        self.model.fc3.zero_grad()\n",
    "        model_output.backward(gradient=one_hot_output, retain_graph=True)\n",
    "        guided_gradients = self.extractor.gradients.data.cpu().numpy()[0]\n",
    "        target = conv_output.data.cpu().numpy()[0]\n",
    "        weights = np.mean(guided_gradients, axis=(1, 2))\n",
    "\n",
    "        cam = np.ones(target.shape[1:], dtype=np.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * target[i, :, :]\n",
    "        print(cam.shape)\n",
    "        cam = cv2.resize(cam, (48, 48))\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))\n",
    "        cam = np.uint8(cam * 255)\n",
    "        return cam\n",
    "            \n",
    "        \n",
    "def save_class_activation_on_image(org_img, activation_map, file_name):\n",
    "    cv2.imwrite(os.path.join(PATH_TMP, file_name+'_Cam_Grayscale.jpg'),\n",
    "               activation_map)\n",
    "    \n",
    "    activation_heatmap = cv2.applyColorMap(activation_map, cv2.COLORMAP_JET)\n",
    "    cv2.imwrite(os.path.join(PATH_TMP, file_name+'_Cam_Heatmap.jpg'),\n",
    "               activation_heatmap)\n",
    "    \n",
    "    img_with_heatmap = np.float32(activation_heatmap) + np.float32(org_img)\n",
    "    img_with_heatmap = img_with_heatmap / np.max(img_with_heatmap)\n",
    "    cv2.imwrite(os.path.join(PATH_TMP, file_name+'_Cam_On_Image.jpg'),\n",
    "               np.uint8(255 * img_with_heatmap))\n",
    "\n",
    "def preprocess_image(np_image):\n",
    "    image = np.expand_dims(np_image, 0)\n",
    "    image = torch.from_numpy(image).float()\n",
    "    image.unsqueeze_(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YM2-zUnyUsEh"
   },
   "outputs": [],
   "source": [
    "def save_class_activation_on_image(org_img, activation_map, file_name):\n",
    "    # cv2.imwrite(os.path.join(PATH_TMP, file_name+'_Cam_Grayscale.jpg'),\n",
    "    #           activation_map)\n",
    "    \n",
    "    return cv2.applyColorMap(activation_map, cv2.COLORMAP_HSV)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJmabgXOHPH8"
   },
   "outputs": [],
   "source": [
    "tmp_image = preprocess_image(fer2013_transformed[187])\n",
    "tmp_image = tmp_image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IIAINxUiHVT-"
   },
   "outputs": [],
   "source": [
    "grad_cam = GradCam(model5, target_layer='conv8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aCYgSwZ0JIvx",
    "outputId": "b01ab898-7562-45b4-f22f-22f52a940cf4"
   },
   "outputs": [],
   "source": [
    "print(tmp_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "colab_type": "code",
    "id": "JPlapnVeIZjn",
    "outputId": "6d9efe88-7178-4182-b7d4-555ead380648"
   },
   "outputs": [],
   "source": [
    "cam = grad_cam.generate_cam(tmp_image, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0G55jsJSwYV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NybbGDHwRt75"
   },
   "outputs": [],
   "source": [
    "heatmap = save_class_activation_on_image(jaffe_transformed[10], cam, 'happy_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "mRFY0eUZSwDd",
    "outputId": "d0d15af0-e75a-4040-97aa-c70dab713fa6"
   },
   "outputs": [],
   "source": [
    "plt.imshow(cam, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "Cc4k2Hb4XEw9",
    "outputId": "913ca16d-39a2-43d6-ef71-9c4bc30c55eb"
   },
   "outputs": [],
   "source": [
    "plt.imshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "r73c7PMi2BKI",
    "outputId": "9d4120ab-d398-426b-8828-ff7e480eb086"
   },
   "outputs": [],
   "source": [
    "plt.imshow(fer2013_transformed[187], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JebeyxMiaCuq"
   },
   "source": [
    "### 6. Using 224 x 224 image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "WCWJifA9zDSv",
    "outputId": "2e8853da-c375-4e16-c117-25f276e6f9c0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fer2013_train = fer2013_array[np.where((fer2013_labels!=1) & (fer2013_labels!=2) & (flags=='Training'))[0]]\n",
    "fer2013_test = fer2013_array[np.where((fer2013_labels!=1) & (fer2013_labels!=2) & (flags=='PrivateTest'))[0]]\n",
    "fer2013_val = fer2013_array[np.where((fer2013_labels!=1) & (fer2013_labels!=2) & (flags=='PublicTest'))[0]]\n",
    "\n",
    "new_labels = {0: 0, 3: 1, 4: 2, 5: 3, 6: 4}\n",
    "\n",
    "fer2013_train_labels = np.array([new_labels[item] \n",
    "                                 for item in \n",
    "                                 fer2013_labels[np.where((fer2013_labels!=1) & (fer2013_labels!=2) & (flags=='Training'))[0]]])\n",
    "fer2013_test_labels = np.array([new_labels[item] \n",
    "                                 for item in \n",
    "                                 fer2013_labels[np.where((fer2013_labels!=1) & (fer2013_labels!=2) & (flags=='PrivateTest'))[0]]])\n",
    "fer2013_val_labels = np.array([new_labels[item] \n",
    "                                 for item in \n",
    "                                 fer2013_labels[np.where((fer2013_labels!=1) & (fer2013_labels!=2) & (flags=='PublicTest'))[0]]])\n",
    "\n",
    "jaffe_train = jaffe_array[np.where((jaffe_labels!=1) & (jaffe_labels!=2))[0]]\n",
    "jaffe_train_labels = np.array([new_labels[item] \n",
    "                                 for item in \n",
    "                                 jaffe_labels[np.where((jaffe_labels!=1) & (jaffe_labels!=2))[0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "QpDPTLa3vXpT",
    "outputId": "e72f8f09-7dfc-462d-b1c6-73098272f3fa"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# get the data in dictionary \n",
    "training_data_dict = {}\n",
    "training_data_jaffe_dict = {}\n",
    "test_data_dict = {}\n",
    "val_data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    training_data_dict[i] = fer2013_train[np.where(fer2013_train_labels==i)]\n",
    "    training_data_jaffe_dict[i] = jaffe_train[np.where(jaffe_train_labels==i)]\n",
    "    test_data_dict[i] = fer2013_test[np.where(fer2013_test_labels==i)]\n",
    "    val_data_dict[i] = fer2013_val[np.where(fer2013_val_labels==i)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "NFtSCHZzxZpu",
    "outputId": "f79d0553-2076-415d-c250-295c073a6e79"
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"------Class {} --------\".format(i))\n",
    "    print(training_data_dict[i].shape)\n",
    "    print(training_data_jaffe_dict[i].shape)\n",
    "    print(test_data_dict[i].shape)\n",
    "    print(val_data_dict[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "mNOlAkLeA-Fp",
    "outputId": "c0591066-3107-45a5-eef7-fe67792b9716"
   },
   "outputs": [],
   "source": [
    "# download the pretrained face_detector file for dlib\n",
    "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 -P /content/data\n",
    "!bzip2 -dk /content/data/shape_predictor_68_face_landmarks.dat.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wokpR8obpdQU"
   },
   "outputs": [],
   "source": [
    "PATH_DETECTOR = '/content/data/shape_predictor_68_face_landmarks.dat'\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(PATH_DETECTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3L5f2C_zGQJr"
   },
   "outputs": [],
   "source": [
    "# Instanciate a face aligner object\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=256)\n",
    "\n",
    "# function to process one image\n",
    "def align_and_crop_one_new(im):\n",
    "\n",
    "    gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
    "    # detect the face in the image\n",
    "    rects = detector(gray, 2)\n",
    "    if len(rects) > 0: # if we have at least one face detected\n",
    "        rects = rects[0] # consider the 1st face\n",
    "        faceAligned = fa.align(im, gray, rects) # align the face\n",
    "\n",
    "        # cropping the image\n",
    "        gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
    "        rects = detector(faceAligned, 2)\n",
    "        if len(rects) > 0:\n",
    "            rects = rects[0]\n",
    "            \n",
    "            (x, y, w, h) = rect_to_bb(rects) # get the bounding box coordinates\n",
    "            \n",
    "            faceCropped = transform.resize(faceAligned[y: y + h, \n",
    "                                                 x: x + w, 0], (224, 224))\n",
    "\n",
    "            return (exposure.equalize_hist(faceCropped), 'OK1')\n",
    "        else:\n",
    "            # if no face is found in the aligned face, return just a resized aligned face 48x48\n",
    "            return (exposure.equalize_hist(transform.resize(faceAligned[:, :, 0], (224, 224))), 'OK2')\n",
    "    else:\n",
    "        # if no face is found at all in the image, return the resized original image 48x48\n",
    "        return (exposure.equalize_hist(transform.resize(im[:, :, 0], (224, 224))), 'OK3')\n",
    "\n",
    "\n",
    "# function to align faces and compute landmarks\n",
    "def align_and_crop_new(list_images, class_, name):\n",
    "    \n",
    "    # converting image to 8-bit\n",
    "    print('--------------Converting images----------------')\n",
    "    list_images = list(np.uint8(list_images))\n",
    "    \n",
    "    #resizing the  images\n",
    "    print('--------------Resizing images----------------')\n",
    "    list_images = [np.repeat(np.expand_dims(im, 2), 3, 2) \n",
    "                       for im in list_images]\n",
    "    \n",
    "    print('--------------Align and Crop----------------')\n",
    "    with Pool() as p:\n",
    "        results_tuple = p.map(align_and_crop_one_new, tqdm.tqdm(list_images))\n",
    "    \n",
    "    results1 = [item[0] for item in results_tuple]\n",
    "    results2 = [item[1] for item in results_tuple]\n",
    "    \n",
    "    # apply histogram equalization to all images and return the result\n",
    "    results1 = np.concatenate([np.expand_dims(item, 0) for item in results1])\n",
    "    \n",
    "    # save the processed images\n",
    "    path = os.path.join('/content/data/', name+'_'+str(class_)+'.npy')\n",
    "    np.save(path, results1)\n",
    "    \n",
    "    return dict(Counter(results2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "MHZANCVp7Wvk",
    "outputId": "0a5a77c5-011b-4a0b-e40d-9276aac3e42d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "counts_j = {}\n",
    "for i in range(5):\n",
    "    print('***********Processing class {}********'.format(i))\n",
    "    counts_j[i] = align_and_crop_new(training_data_jaffe_dict[i], i, 'jaffe_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "bD_n1hh37eGx",
    "outputId": "196338c8-7555-4df7-db59-9668084a5815"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "counts_test = {}\n",
    "for i in range(5):\n",
    "    print('***********Processing class {}********'.format(i))\n",
    "    counts_test[i] = align_and_crop_new(test_data_dict[i], i, 'fer2013_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "2N4CD-l2-GbQ",
    "outputId": "a6f95211-7d5c-4f1d-d471-c9fe29dd9c7f"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "counts_val = {}\n",
    "for i in range(5):\n",
    "    print('***********Processing class {}********'.format(i))\n",
    "    counts_val[i] = align_and_crop_new(val_data_dict[i], i, 'fer2013_val')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "9BaaeIixDqHT",
    "outputId": "c335039b-21e8-4071-a8bf-5641066d1e1e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!cp /content/data/*.npy /content/gdrive/My\\ Drive/Face_detection/data_224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "WqfGBM42EOtj",
    "outputId": "81b3d4af-1afe-4bd0-8c69-43de62248767"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "counts_train = {}\n",
    "for i in range(5):\n",
    "    print('***********Processing class {}********'.format(i))\n",
    "    counts_train[i] = align_and_crop_new(training_data_dict[i], i, 'fer2013_train')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3e_-Sc-IsE6B"
   },
   "outputs": [],
   "source": [
    "dirs = ['train', 'val', 'test']\n",
    "classes = ['0', '1', '2', '3', '4']\n",
    "p_drive = '/content/gdrive/My Drive/Face_detection/data_224/'\n",
    "for d in dirs:\n",
    "    for cl in classes:\n",
    "        if not os.path.exists(os.path.join(p_drive, d)):\n",
    "            os.mkdir(os.path.join(p_drive, d))\n",
    "        if not os.path.exists(os.path.join(os.path.join(p_drive, d), cl)):\n",
    "            os.mkdir(os.path.join(os.path.join(p_drive, d), cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CP6pr3UvwWlT"
   },
   "outputs": [],
   "source": [
    "def save_as_image(paths):\n",
    "    data = []\n",
    "    for p in paths:\n",
    "        data.append(np.load(p))\n",
    "    cl = paths[0].split('_')[-1][:-4]\n",
    "    d = paths[0].split('_')[-2]\n",
    "    data = np.concatenate(data)\n",
    "    \n",
    "    for i, im in tqdm.tqdm(enumerate(data)):\n",
    "        plt.imsave(arr=im, fname=os.path.join(p_drive, d, cl, str(i)+'.png'), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "xm9n9fk9vfcQ",
    "outputId": "bfbe1246-d612-4a8d-851d-62fc5131e944"
   },
   "outputs": [],
   "source": [
    "list_paths = sorted(glob.glob('/content/data/*.npy'))\n",
    "list_paths_test_val = [[p] for p in list_paths if 'train' not in p]\n",
    "list_paths_train = [[p1, p2] \n",
    "                    for p1, p2 in zip([p for p in list_paths if 'fer2013_train' in p],\n",
    "                                     [p for p in list_paths if 'jaffe_train' in p])]\n",
    "print(list_paths_train)\n",
    "print(list_paths_test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "Vd6xHDsLyK5V",
    "outputId": "dcc42b06-296f-4e77-94e7-d9cfee5e4a0a"
   },
   "outputs": [],
   "source": [
    "for i, p in enumerate(list_paths_train):\n",
    "    print('Saving class {}'.format(i))\n",
    "    save_as_image(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "tz1Vp0wwJzBU",
    "outputId": "8463cc37-8b39-46c3-a62b-d1e8282e4fbd"
   },
   "outputs": [],
   "source": [
    "for p in list_paths_test_val:\n",
    "    print('*******{}*******'.format(p))\n",
    "    save_as_image(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "IDJ1puUkzw0M",
    "outputId": "368d7290-d23e-46b5-984f-88ba3f59cec3"
   },
   "outputs": [],
   "source": [
    "list_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VFDYPK-5z6fS"
   },
   "outputs": [],
   "source": [
    "tmp2 = np.concatenate([tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "AFZsoHafz-Td",
    "outputId": "b45897a2-f27c-44f7-c82d-2c40e3cc876f"
   },
   "outputs": [],
   "source": [
    "tmp2 == tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "3geEZjw0dl57",
    "outputId": "7f82b90b-3264-44cd-cdd5-565a88a849d9"
   },
   "outputs": [],
   "source": [
    "! du -ah /content/data/*train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NPu2gDZ5k_j1",
    "outputId": "3bf77251-35b6-4aca-910e-e98ccb6252c2"
   },
   "outputs": [],
   "source": [
    "1.5+2.75+1.9+1.2+1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "2P5o2E2wjr6g",
    "outputId": "7942b8b7-b2fe-4ae6-bdfc-2f97ba5b5384"
   },
   "outputs": [],
   "source": [
    "!ls /content/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "s2LAOFgTEeTm",
    "outputId": "3604d745-6e89-4cd7-991c-db5a149428c6"
   },
   "outputs": [],
   "source": [
    "print(counts_val)\n",
    "print(counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "puUUHVWir3rr",
    "outputId": "387229c3-35db-4eb8-be47-416e8724e42a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fer2013_transformed, counts = align_and_crop_new(fer2013_array)\n",
    "jaffe_transformed, counts_j = align_and_crop_new(jaffe_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVucoris67aD"
   },
   "outputs": [],
   "source": [
    "fer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLlgdlZcS_3_"
   },
   "outputs": [],
   "source": [
    "tmp_image = np.repeat(np.expand_dims(fer2013_array[2], 2), 3, 2).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Noi4LOp3p95H"
   },
   "outputs": [],
   "source": [
    "tmp_image2, ok = align_and_crop_one_new(tmp_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1x0ltuCOrCRz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "Ir9GTqaiqX_M",
    "outputId": "e8469402-fc04-4b4b-bafa-cd56b5f5b804"
   },
   "outputs": [],
   "source": [
    "plt.imshow(tmp_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "GIMGYo3HTgou",
    "outputId": "6e8044c4-eaef-4f9b-8411-191cf81b7879"
   },
   "outputs": [],
   "source": [
    "plt.imshow(tmp_image2, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oO2_0Z2YaI6o"
   },
   "outputs": [],
   "source": [
    "# loading data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkUyG8J3ansZ"
   },
   "source": [
    "###  5. Transfert learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfNJCjP8bqra"
   },
   "source": [
    "#### 5.1 ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLA5nMELayQk"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1476
    },
    "colab_type": "code",
    "id": "QblB30HvcXZ0",
    "outputId": "96e05d57-1190-48c5-fb90-1c357d1447c2"
   },
   "outputs": [],
   "source": [
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLC79e-DT7z_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWBKvQ5QT7um"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9evUvXC7T7od"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Tm8ppWJT7id"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IH2dS5PhT7ao"
   },
   "outputs": [],
   "source": [
    "class ModelTest(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout=0.3):\n",
    "        super(ModelTest, self).__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # 1st block\n",
    "        self.conv1 = SeparableConv(1, 64)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(64)\n",
    "        self.conv2 = SeparableConv(64, 64)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(64)\n",
    "        \n",
    "        # 2nd block\n",
    "        self.conv3 = SeparableConv(64, 128)\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(128)\n",
    "        self.conv4 = SeparableConv(128, 128)\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(128)\n",
    "        \n",
    "        # 3rd block\n",
    "        self.conv5 = SeparableConv(128, 256)\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(256)\n",
    "        self.conv6 = SeparableConv(256, 256)\n",
    "        self.batchnorm6 = torch.nn.BatchNorm2d(256)\n",
    "        \n",
    "        # 4th block\n",
    "        self.conv7 = SeparableConv(256, 512)\n",
    "        self.batchnorm7 = torch.nn.BatchNorm2d(512)\n",
    "        self.conv8 = SeparableConv(512, 512)\n",
    "        self.batchnorm8 = torch.nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 1st fc block\n",
    "        self.fc1 = torch.nn.Linear(512, 7)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #1st block\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # 2nd block\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.batchnorm3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.batchnorm4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # 3rd block\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.batchnorm5(x))\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(self.batchnorm6(x))\n",
    "        # x = F.max_pool2d(x, 2)\n",
    "                \n",
    "        # 4th block\n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(self.batchnorm7(x))\n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(self.batchnorm8(x))\n",
    "        # x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(-1, x.size()[1])\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNe6Mk0kUkLQ"
   },
   "outputs": [],
   "source": [
    "# instantiate model and optimizers\n",
    "model5 = ModelTest()\n",
    "model5 = model5.to(device)\n",
    "criterion5 = torch.nn.CrossEntropyLoss()\n",
    "optimizer5 = optim.Adam(model5.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4393
    },
    "colab_type": "code",
    "id": "KoUgTbfaU4PC",
    "outputId": "05572bb4-20d3-471f-d0a2-45670c958b92"
   },
   "outputs": [],
   "source": [
    "model5, accuracies5, losses5 = train_model(model5, criterion5, optimizer5, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YMzfS1TnVWCC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EmotionDetection.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
